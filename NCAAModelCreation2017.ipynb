{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>NCAA 2017 Basketball Predictive Model Creation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note book loads <b>statsdict</b> from the p file created by <i>NCAAModelSetup.ipynb</i> to help create a predictive model to find the Probabilities of future NCAA matchups. The model uses Logistic Regression, and takes two vectors, <b>X_train</b> and <b>y_train</b>, which are respectively our Features and Labels. The features are the average stats of two teams from previous games, and labels are either <b>1</b> or <b>0</b>. <b>1</b> Indicates the first team won, while, <b>0</b> indicates the second team won. After words a submission file compatible with Kaggle is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "from sklearn import linear_model, metrics, neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regseasons_d = pd.read_csv(\"2017_Data/RegularSeasonDetailedResults.csv\")\n",
    "tourney_d = pd.read_csv(\"2017_Data/TourneyDetailedResults.csv\")\n",
    "teams = pd.read_csv(\"2017_Data/Teams.csv\")\n",
    "regseasons_d['GameType'] = 'S'\n",
    "tourney_d['GameType'] = 'T'\n",
    "games_d = pd.concat([regseasons_d, tourney_d])\n",
    "games_d= games_d.sort_values(['Season','Daynum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = ['Wscore', 'Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr',\n",
    "        'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf', 'Lscore', 'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3',\n",
    "        'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf',\n",
    "        'Wfgp', 'Lfgp', 'Wfgp3', 'Lfgp3', 'Wftp', 'Lftp']\n",
    "stats.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting the stat labels for each respective team\n",
    "team1_stats = (stats[int(len(stats)/2):]) # Team 1 is the Winning Team\n",
    "team2_stats = (stats[0:int(len(stats)/2)]) # Team 2 is the Losing Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wast', 'Wblk', 'Wdr', 'Wfga', 'Wfga3', 'Wfgm', 'Wfgm3', 'Wfgp', 'Wfgp3', 'Wfta', 'Wftm', 'Wftp', 'Wor', 'Wpf', 'Wscore', 'Wstl', 'Wto'] 17\n",
      "['Last', 'Lblk', 'Ldr', 'Lfga', 'Lfga3', 'Lfgm', 'Lfgm3', 'Lfgp', 'Lfgp3', 'Lfta', 'Lftm', 'Lftp', 'Lor', 'Lpf', 'Lscore', 'Lstl', 'Lto'] 17\n"
     ]
    }
   ],
   "source": [
    "#Display the stat label for each team, followed by the length of the label list\n",
    "print(team1_stats,len(team2_stats))\n",
    "print(team2_stats,len(team1_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Our Stats Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statsdict = pickle.load(open('statsdict.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_K(wteam_elo, lteam_elo, wteam_score, lteam_score):\n",
    "    diff = wteam_score - lteam_score\n",
    "    if(diff>25):\n",
    "        return 30\n",
    "    if(diff>15):\n",
    "        return 25\n",
    "    return 20\n",
    "\n",
    "def new_elo(wteam_elo=1600, lteam_elo=1600, wteam_score=80, lteam_score=80):\n",
    "    eloDiff = wteam_elo - lteam_elo\n",
    "    odds = 1 / (10**(-eloDiff/400) + 1)\n",
    "    K = get_K(wteam_elo, lteam_elo, wteam_score, lteam_score)\n",
    "    diff = round(K * (1-odds))\n",
    "    return (wteam_elo + diff, lteam_elo - diff)\n",
    "\n",
    "def get_elo(season, teamnum, game_day=-1):\n",
    "    if (season,teamnum) in statsdict and ('Elo' in statsdict[season,teamnum]):\n",
    "        if game_day<0:\n",
    "            return statsdict[season,teamnum]['Elo'][-1]\n",
    "        else:\n",
    "            gameNum = statsdict[season, teamnum]['GameDict'][game_day]\n",
    "            return statsdict[season,teamnum]['Elo'][gameNum]\n",
    "    elif ((season-1,teamnum) in statsdict) and ('Elo' in statsdict[season-1,teamnum]):\n",
    "        return (1600*(1/4)) + (3/4)*statsdict[season-1,teamnum]['Elo'][-1]\n",
    "    else:\n",
    "        return 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up a Stat Average Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Returns the average stat for a specified team based on the name and what day their playing on\n",
    "#If day is not specified gets the average stats from the last day\n",
    "def get_stat_average(stat, season, teamnum, gamenum=-1, ngames=4):\n",
    "    if(gamenum==1):\n",
    "        return -1\n",
    "    if(gamenum<0):\n",
    "        return get_stat_average(stat, season, teamnum,len(statsdict[season, teamnum][stat])+1, ngames)\n",
    "    avg = np.nanmean(statsdict[season, teamnum][stat][max(0, gamenum-ngames-1):gamenum-1])\n",
    "    if np.isnan(avg):\n",
    "        return -1\n",
    "    else:\n",
    "        return avg  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:703: RuntimeWarning: Mean of empty slice\n",
      "  warnings.warn(\"Mean of empty slice\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "tourney_indexes=[]\n",
    "for index, game in games_d[(games_d.Season<=2015)&(games_d.Season>=2003)].iterrows():\n",
    "    wteam_statrow = []\n",
    "    lteam_statrow = []\n",
    "    skip_game = False\n",
    "    gameNum = statsdict[game['Season'], game.Wteam]['GameDict'][game.Daynum]\n",
    "    for stat in team1_stats:\n",
    "        avg_stat =get_stat_average(stat[1:],game['Season'],game.Wteam,gameNum,ngames=7)\n",
    "        if(stat in ['Wfgp', 'Wfgp3', 'Wftp']):\n",
    "            avg_stat = avg_stat*1\n",
    "        wteam_statrow.append(avg_stat)\n",
    "        if(avg_stat<0):\n",
    "            skip_game = True\n",
    "    wteam_statrow.append(get_elo(game['Season'], game['Wteam'],game.Daynum))\n",
    "    \n",
    "    gameNum = statsdict[game['Season'], game.Lteam]['GameDict'][game.Daynum]\n",
    "    for stat in team2_stats:\n",
    "        avg_stat =get_stat_average(stat[1:],game['Season'],game.Lteam,gameNum,ngames=7)\n",
    "        if(stat in ['Lfgp', 'Lfgp3', 'Lftp']):\n",
    "            avg_stat = avg_stat*1\n",
    "        lteam_statrow.append(avg_stat)\n",
    "        if(avg_stat<0):\n",
    "            skip_game = True            \n",
    "    lteam_statrow.append(get_elo(game['Season'], game['Lteam'], game.Daynum))\n",
    "    if skip_game:\n",
    "        continue\n",
    "    if game.GameType=='T':\n",
    "        tourney_indexes.append(len(X))\n",
    "        tourney_indexes.append(len(X)+1)\n",
    "    row1 = []\n",
    "    row1.extend(wteam_statrow)\n",
    "    row1.extend(lteam_statrow)\n",
    "    row2 = []\n",
    "    row2.extend(lteam_statrow)\n",
    "    row2.extend(wteam_statrow)\n",
    "    X.append(row1)\n",
    "    X.append(row2)\n",
    "    y.append(1)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tourney_indexes2 = tourney_indexes[-(69*4):]\n",
    "season_indexes = list(filter(lambda t: t not in tourney_indexes2, range(len(X))))\n",
    "X_train = [X[s] for s in season_indexes]\n",
    "X_test = [X[t] for t in tourney_indexes2]\n",
    "y_train = [y[s] for s in season_indexes]\n",
    "y_test = [y[t] for t in tourney_indexes2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=300,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = linear_model.LogisticRegressionCV(max_iter = 300)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.826086956522\n",
      "Log Loss: 0.418535822257\n"
     ]
    }
   ],
   "source": [
    "#Model 1\n",
    "print('Score', model.score((X_test), y_test))\n",
    "print('Log Loss:', metrics.log_loss(y_test, model.predict_proba((X_test))[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.826086956522\n",
      "Log Loss: 0.41690919678\n"
     ]
    }
   ],
   "source": [
    "#Model 2\n",
    "print('Score', model2.score((X_test), y_test))\n",
    "print('Log Loss:', metrics.log_loss(y_test, model2.predict_proba((X_test))[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Model to p file for Later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model2017.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_last_gamenum(season, teamnum, game_day=-1):\n",
    "    day_key_list = list(statsdict[season, teamnum]['GameDict'].keys())\n",
    "    day_key_list.sort()\n",
    "    for d in day_key_list:\n",
    "        if d >= game_day:\n",
    "            return statsdict[season, teamnum]['GameDict'][d]\n",
    "    return statsdict[season, teamnum]['GameDict'][day_key_list[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "teams = pd.read_csv('2017_Data/Teams.csv')\n",
    "t_seeds = pd.read_csv('2017_Data/TourneySeeds.csv')\n",
    "arr = []\n",
    "\n",
    "for season in [2016,2017]:\n",
    "    for index, seed1 in t_seeds[t_seeds.Season==season].iterrows():\n",
    "        for index2, seed2 in t_seeds[t_seeds.Season==season].loc[index+1:].iterrows():\n",
    "            arr.append((seed1.Team, seed2.Team, season))\n",
    "\n",
    "submission = []\n",
    "\n",
    "for team1,team2,season in arr:\n",
    "    X_matchup = []\n",
    "    for stat in team1_stats:\n",
    "        gamenum = get_last_gamenum(season, team1, 134)\n",
    "        avg_stat = get_stat_average(stat[1:], season, team1, gamenum=gamenum, ngames=7)\n",
    "        X_matchup.append(avg_stat)\n",
    "    X_matchup.append(get_elo(season, team1))\n",
    "    for stat in team2_stats:\n",
    "        gamenum = get_last_gamenum(season, team2, 134)\n",
    "        avg_stat = get_stat_average(stat[1:], season, team2, gamenum=gamenum, ngames=7)\n",
    "        X_matchup.append(avg_stat)\n",
    "    X_matchup.append(get_elo(season, team2))\n",
    "    submission.append([team1,team2, model2.predict_proba([X_matchup]),season])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Matchup: 4556\n"
     ]
    }
   ],
   "source": [
    "print('Size of Matchup:', len(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('Submission_2017.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Id', 'Pred']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in submission:\n",
    "        pa = row[2][0][0]\n",
    "        pb = row[2][0][1]\n",
    "        writer.writerow({'Id': str(row[3])+'_'+str(min(row[0],row[1]))+'_'+str(max(row[0],row[1])), \n",
    "                         'Pred': (lambda r: pa if r[0] > r[1] else pb)(row)})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
